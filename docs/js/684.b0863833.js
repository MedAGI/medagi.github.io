"use strict";(self["webpackChunkmiccai2023"]=self["webpackChunkmiccai2023"]||[]).push([[684],{3684:function(a,e,i){i.r(e),i.d(e,{default:function(){return ta}});var t=i(6768),s=i(4232);const d=a=>((0,t.Qi)("data-v-1da4b46f"),a=a(),(0,t.jt)(),a),n={id:"submission"},l=d((()=>(0,t.Lk)("div",{class:"main-title"},[(0,t.Lk)("h1",null,"Call For Paper"),(0,t.Lk)("div",{class:"main-title-deco"})],-1))),r={key:0},o=d((()=>(0,t.Lk)("div",{id:"scope"},[(0,t.Lk)("h2",null,"Scope of the Workshop"),(0,t.Lk)("hr"),(0,t.Lk)("div",{class:"scope-contents"}," In the field of medical image analysis, existing AI solutions are typically designed and evaluated on specific datasets, making it challenging to transfer them to different tasks or handle datasets from diverse medical centers. However, real-world clinical practices vary across hospitals and institutions, resulting in different data modalities and task formulations. As a result, there is increasing attention toward developing a general model that can effectively handle various medical scenarios. Such a model, with excellent generalization ability to process different medical image modalities and perform a variety of medical AI tasks, is commonly referred to as a general medical AI. Inspired by recent advances in the generalization power of large-scale vision-language and foundation models in computer vision, this first workshop on foundation models for general medical AI (MedAGI) is dedicated to addressing the current medical AI systems and discussing opportunities for generalizing learning systems across multiple unseen tasks and domains specifically targeting various medical data (e.g., images, omics, clinical data) processing and analysis scenarios. ")],-1))),c=[o],v={key:1},u=d((()=>(0,t.Lk)("div",{id:"scope"},[(0,t.Lk)("h2",null,"Scope of the Workshop"),(0,t.Lk)("hr"),(0,t.Lk)("div",{class:"scope-contents"}," Medical image analysis has traditionally relied on AI models trained on specific datasets, which often becomes challenging when transferred to data from different medical centers. This inherent limitation has inspired a growing interest in general medical AI, capable of seamlessly adapting to various medical scenarios, data modalities, and task formulations prevalent across hospitals and institutions. Drawing parallels from the computer vision and natural language processing domains, foundation models, such as large language and vision-language models like GPT, LLaMA, stand out as quintessential general AI solutions. These models have demonstrated remarkable proficiency in a myriad of tasks owing to their massive training datasets and substantial model sizes. Yet, the translation of these successes to medical, namely the general medical AI, remains nascent. This workshop is designed to continue the success of our last year’s event and serve as a confluence of insights from the current landscape of medical AI and foundation models. This year, we aim to foster discussions that will pave the way for the evolution of task-specific medical AI systems into more generalized frameworks capable of tackling a diverse range of tasks, datasets, and domains. ")],-1))),f=[u],m={key:2},b=(0,t.Fv)('<div id="topics" data-v-1da4b46f><h2 data-v-1da4b46f>Topics</h2><hr data-v-1da4b46f><div id="topic-description" data-v-1da4b46f> Topics are included but not limited to: </div><ul id="topic-list" data-v-1da4b46f><li data-v-1da4b46f>AGI, Foundation models, and multi-purpose models for medical data</li><li data-v-1da4b46f>Benchmarks for general medical AI</li><li data-v-1da4b46f>Zero/few-shot, self-/weak-supervised learning</li><li data-v-1da4b46f>Domain generalization and adaptation</li><li data-v-1da4b46f>Pre-training, fine-tuning, prompt tuning</li><li data-v-1da4b46f>Open-vocabulary object detection, segmentation</li><li data-v-1da4b46f>Multitask learning, knowledge distillation, pseudo labeling</li><li data-v-1da4b46f>Current challenges and/or future trends in general medical AI</li><li data-v-1da4b46f>Social impact and/or ethical issues of general medical AI</li></ul></div>',1),p=[b],h={key:3},g=(0,t.Fv)('<div id="topics" data-v-1da4b46f><h2 data-v-1da4b46f>Topics</h2><hr data-v-1da4b46f><div id="topic-description" data-v-1da4b46f> Topics are included but not limited to: </div><ul id="topic-list" data-v-1da4b46f><li data-v-1da4b46f>Medical artificial general intelligence</li><li data-v-1da4b46f>Medical foundation models</li><li data-v-1da4b46f>Medical generative AI</li><li data-v-1da4b46f>Medical large language/vision models</li><li data-v-1da4b46f>Medical vision-language models</li><li data-v-1da4b46f>Benchmarking and evaluations for general medical AI</li><li data-v-1da4b46f>Current challenges and future trends in general medical AI</li><li data-v-1da4b46f>Social impact and ethical considerations in general medical AI</li></ul></div>',1),k=[g],w=d((()=>(0,t.Lk)("h2",null,"Important Dates",-1))),y=d((()=>(0,t.Lk)("hr",null,null,-1))),L={key:4},A=(0,t.Fv)('<div id="important-dates" data-v-1da4b46f><div id="dates-table" data-v-1da4b46f><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f><span class="canceled-date" data-v-1da4b46f>June 25, 2023 (23:59 PDT)</span><br data-v-1da4b46f>July 5, 2023 (23:59PDT)</div><div class="due-work" data-v-1da4b46f>Paper submission due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f><span class="canceled-date" data-v-1da4b46f>July 30, 2023</span><br data-v-1da4b46f>August 6, 2023</div><div class="due-work" data-v-1da4b46f>Notification of paper decisions</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f><span class="canceled-date" data-v-1da4b46f>August 13, 2023</span><br data-v-1da4b46f>August 20, 2023</div><div class="due-work" data-v-1da4b46f>Camera ready papers due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>September 6, 2023</div><div class="due-work" data-v-1da4b46f>Workshop proceedings due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>October 12, 2023</div><div class="due-work" data-v-1da4b46f>Workshop date</div></div></div></div>',1),I=[A],C={key:5},M=(0,t.Fv)('<div id="important-dates" data-v-1da4b46f><div id="dates-table" data-v-1da4b46f><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>May 1, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Paper submission open</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>June 23, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Paper submission due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>July 14, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Review Due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>July 21, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Final Notification</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>August 4, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Camera-ready Due</div></div><div class="date-row" data-v-1da4b46f><div class="dates" data-v-1da4b46f>October 6, 2024<br data-v-1da4b46f></div><div class="due-work" data-v-1da4b46f>Workshop Date</div></div></div></div>',1),S=[M],W={id:"instructions"},T=d((()=>(0,t.Lk)("h2",null,"Submission Instructions",-1))),P=d((()=>(0,t.Lk)("hr",null,null,-1))),E=d((()=>(0,t.Lk)("div",{class:"instruction-contents"},[(0,t.Lk)("strong",null,"Submission format |"),(0,t.eW)(" Submissions are in two tracks: Full-length papers and extended abstracts. "),(0,t.Lk)("ol",null,[(0,t.Lk)("li",null,[(0,t.eW)(" Full-length papers: Manuscripts should be up to "),(0,t.Lk)("b",null,"8 pages"),(0,t.eW)(" (text, figures, and tables) plus up to "),(0,t.Lk)("b",null,"2 pages"),(0,t.eW)(" of references. Submissions must be new. Accepted papers will be assigned as oral or poster presentations. ")]),(0,t.Lk)("li",null,[(0,t.eW)(" Extended abstracts: Manuscripts should be up to "),(0,t.Lk)("b",null,"2 pages"),(0,t.eW)(" (text, figures, tables, and references). Submissions may be new work or recently published/accepted papers (including posted preprints). Accepted abstracts will be assigned primarily as poster presentations. Accepted abstracts will not be formally published by publishers, but the authors can choose to make them archived on the workshop website. ")])])],-1))),D={class:"instruction-contents"},F=d((()=>(0,t.Lk)("strong",null,"Manuscript template | ",-1))),X=d((()=>(0,t.Lk)("a",{target:"_blank",href:"https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines"},"Lecture Notes in Computer Science",-1))),Y=d((()=>(0,t.Lk)("div",{class:"instruction-contents"},[(0,t.Lk)("strong",null,"Breach of anonymization | "),(0,t.eW)("The review process is double-blind, i.e. the names of the authors, reviewers, and area chairs are not revealed to each other. Papers must thus be properly anonymized before submission. ")],-1))),z=d((()=>(0,t.Lk)("div",{class:"instruction-contents"},[(0,t.Lk)("strong",null,"Domain conflicts | "),(0,t.eW)("To avoid conflict of interest among the authors, reviewers, and meta-reviewers, all co-author information and a complete and accurate list of domain conflicts must be entered in the submission form by the submission deadline. For the same reason, i.e., to avoid conflict of interest during the review process, the author list must be complete at submission time. ")],-1))),G={key:0},x=d((()=>(0,t.Lk)("div",{class:"instruction-contents"},[(0,t.Lk)("strong",null,"Publication | "),(0,t.eW)("Accepted full-length papers will be published with MICCAI Proceedings in the Springer LNCS Series. Expanded versions of selected papers will be invited to a special issue “Foundation Models for Medical Image Analysis” of the Medical Image Analysis journal. ")],-1))),_=[x],J={key:1},N=d((()=>(0,t.Lk)("div",{class:"instruction-contents"},[(0,t.Lk)("strong",null,"Publication | "),(0,t.eW)("Accepted full-length papers are tentatively planned to be published with the MICCAI Proceedings in the Springer LNCS Series. ")],-1))),j=[N],q={id:"awards"},B=d((()=>(0,t.Lk)("h2",null,"Awards",-1))),O=d((()=>(0,t.Lk)("hr",null,null,-1))),H={class:"awards-contents"},Q={key:2},R=d((()=>(0,t.Lk)("div",{id:"submission-site"},[(0,t.Lk)("h2",null,"Submission Site"),(0,t.Lk)("hr"),(0,t.Lk)("a",{href:"https://cmt3.research.microsoft.com/MedAGI2023",target:"_blank"},"https://cmt3.research.microsoft.com/MedAGI2023 ")],-1))),Z=[R],$={key:3},K=d((()=>(0,t.Lk)("div",{id:"submission-site"},[(0,t.Lk)("h2",null,"Submission Site"),(0,t.Lk)("hr"),(0,t.Lk)("a",{href:"https://cmt3.research.microsoft.com/MedAGI2024",target:"_blank"},"https://cmt3.research.microsoft.com/MedAGI2024 ")],-1))),U=[K];function V(a,e,i,d,o,u){return(0,t.uX)(),(0,t.CE)("div",n,[l,"2023"===u.currentYear?((0,t.uX)(),(0,t.CE)("div",r,c)):((0,t.uX)(),(0,t.CE)("div",v,f)),"2023"===u.currentYear?((0,t.uX)(),(0,t.CE)("div",m,p)):((0,t.uX)(),(0,t.CE)("div",h,k)),w,y,"2023"===u.currentYear?((0,t.uX)(),(0,t.CE)("div",L,I)):((0,t.uX)(),(0,t.CE)("div",C,S)),(0,t.Lk)("div",W,[T,P,(0,t.Lk)("div",null,[E,(0,t.Lk)("div",D,[F,(0,t.eW)(" In general, the format requirements are the same as MICCAI "+(0,s.v_)(u.currentYear)+" main conference. No modifications to the templates are permitted. Papers must be submitted electronically in searchable pdf format following the guidelines for authors and LaTeX and MS Word templates available at ",1),X,(0,t.eW)(". ")]),Y,z,"2023"===u.currentYear?((0,t.uX)(),(0,t.CE)("div",G,_)):((0,t.uX)(),(0,t.CE)("div",J,j)),(0,t.Lk)("div",q,[B,O,(0,t.Lk)("div",H," The MedAGI "+(0,s.v_)(u.currentYear)+" best paper and the honorable mention awards will be given to the two high-quality papers chosen by the award committee. ",1)]),"2023"===u.currentYear?((0,t.uX)(),(0,t.CE)("div",Q,Z)):((0,t.uX)(),(0,t.CE)("div",$,U))])])])}var aa={computed:{currentYear(){return this.$route.params.year||(new Date).getFullYear().toString()}},methods:{fetchData(){this.currentYear}},created(){this.fetchData()}},ea=i(1241);const ia=(0,ea.A)(aa,[["render",V],["__scopeId","data-v-1da4b46f"]]);var ta=ia}}]);
//# sourceMappingURL=684.b0863833.js.map