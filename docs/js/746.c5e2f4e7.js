"use strict";(self["webpackChunkmiccai2023"]=self["webpackChunkmiccai2023"]||[]).push([[746],{8746:function(a,e,t){t.r(e),t.d(e,{default:function(){return h}});var d=t(3396);const i={id:"submission"},s=(0,d.uE)('<div class="main-title" data-v-0730d3ca><h1 data-v-0730d3ca>Call For Paper</h1><div class="main-title-deco" data-v-0730d3ca></div></div><div id="scope" data-v-0730d3ca><h2 data-v-0730d3ca>Scope of the Workshop</h2><hr data-v-0730d3ca><div class="scope-contents" data-v-0730d3ca> In the field of medical image analysis, existing AI solutions are typically designed and evaluated on specific datasets, making it challenging to transfer them to different tasks or handle datasets from diverse medical centers. However, real-world clinical practices vary across hospitals and institutions, resulting in different data modalities and task formulations. As a result, there is increasing attention toward developing a general model that can effectively handle various medical scenarios. Such a model, with excellent generalization ability to process different medical image modalities and perform a variety of medical AI tasks, is commonly referred to as a general medical AI. Inspired by recent advances in the generalization power of large-scale vision-language and foundation models in computer vision, this first workshop on foundation models for general medical AI (MedAGI) is dedicated to addressing the current medical AI systems and discussing opportunities for generalizing learning systems across multiple unseen tasks and domains specifically targeting various medical data (e.g., images, omics, clinical data) processing and analysis scenarios. </div></div><div id="topics" data-v-0730d3ca><h2 data-v-0730d3ca>Topics</h2><hr data-v-0730d3ca><div id="topic-description" data-v-0730d3ca> Topics are included but not limited to: </div><ul id="topic-list" data-v-0730d3ca><li data-v-0730d3ca>AGI, Foundation models, and multi-purpose models for medical data</li><li data-v-0730d3ca>Benchmarks for general medical AI</li><li data-v-0730d3ca>Zero/few-shot, self-/weak-supervised learning</li><li data-v-0730d3ca>Domain generalization and adaptation</li><li data-v-0730d3ca>Pre-training, fine-tuning, prompt tuning</li><li data-v-0730d3ca>Open-vocabulary object detection, segmentation</li><li data-v-0730d3ca>Multitask learning, knowledge distillation, pseudo labeling</li><li data-v-0730d3ca>Current challenges and/or future trends in general medical AI</li><li data-v-0730d3ca>Social impact and/or ethical issues of general medical AI</li></ul></div><h2 data-v-0730d3ca>Important Dates</h2><hr data-v-0730d3ca>',5),n={key:0},c=(0,d.uE)('<div id="important-dates" data-v-0730d3ca><div id="dates-table" data-v-0730d3ca><div class="date-row" data-v-0730d3ca><div class="dates" data-v-0730d3ca><span class="canceled-date" data-v-0730d3ca>June 25, 2023 (23:59 PDT)</span><br data-v-0730d3ca>July 5, 2023 (23:59PDT)</div><div class="due-work" data-v-0730d3ca>Paper submission due</div></div><div class="date-row" data-v-0730d3ca><div class="dates" data-v-0730d3ca><span class="canceled-date" data-v-0730d3ca>July 30, 2023</span><br data-v-0730d3ca>August 6, 2023</div><div class="due-work" data-v-0730d3ca>Notification of paper decisions</div></div><div class="date-row" data-v-0730d3ca><div class="dates" data-v-0730d3ca><span class="canceled-date" data-v-0730d3ca>August 13, 2023</span><br data-v-0730d3ca>August 20, 2023</div><div class="due-work" data-v-0730d3ca>Camera ready papers due</div></div><div class="date-row" data-v-0730d3ca><div class="dates" data-v-0730d3ca>September 6, 2023</div><div class="due-work" data-v-0730d3ca>Workshop proceedings due</div></div><div class="date-row" data-v-0730d3ca><div class="dates" data-v-0730d3ca>October 12, 2023</div><div class="due-work" data-v-0730d3ca>Workshop date</div></div></div></div>',1),r=[c],o={key:1,style:{"font-size":"35px"}},l=(0,d.uE)('<div id="instructions" data-v-0730d3ca><h2 data-v-0730d3ca>Submission Instructions</h2><hr data-v-0730d3ca><div data-v-0730d3ca><div class="instruction-contents" data-v-0730d3ca><strong data-v-0730d3ca>Submission format |</strong> Submissions are in two tracks: Full-length papers and extended abstracts. <ol data-v-0730d3ca><li data-v-0730d3ca> Full-length papers: Manuscripts should be up to <b data-v-0730d3ca>8 pages</b> (text, figures, and tables) plus up to <b data-v-0730d3ca>2 pages</b> of references. Submissions must be new. Accepted papers will be assigned as oral or poster presentations. </li><li data-v-0730d3ca> Extended abstracts: Manuscripts should be up to <b data-v-0730d3ca>2 pages</b> (text, figures, tables, and references). Submissions may be new work or recently published/accepted papers (including posted preprints). Accepted abstracts will be assigned primarily as poster presentations. Accepted abstracts will not be formally published by publishers, but the authors can choose to make them archived on the workshop website. </li></ol></div><div class="instruction-contents" data-v-0730d3ca><strong data-v-0730d3ca>Manuscript template | </strong> In general, the format requirements are the same as MICCAI 2023 main conference. No modifications to the templates are permitted. Papers must be submitted electronically in searchable pdf format following the guidelines for authors and LaTeX and MS Word templates available at <a target="_blank" href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" data-v-0730d3ca>Lecture Notes in Computer Science</a>. </div><div class="instruction-contents" data-v-0730d3ca><strong data-v-0730d3ca>Breach of anonymization | </strong>The review process is double-blind, i.e. the names of the authors, reviewers, and area chairs are not revealed to each other. Papers must thus be properly anonymized before submission. </div><div class="instruction-contents" data-v-0730d3ca><strong data-v-0730d3ca>Domain conflicts | </strong>To avoid conflict of interest among the authors, reviewers, and meta-reviewers, all co-author information and a complete and accurate list of domain conflicts must be entered in the submission form by the submission deadline. For the same reason, i.e., to avoid conflict of interest during the review process, the author list must be complete at submission time. </div><div class="instruction-contents" data-v-0730d3ca><strong data-v-0730d3ca>Publication | </strong>Accepted full-length papers will be published with MICCAI Proceedings in the Springer LNCS Series. Expanded versions of selected papers will be invited to a special issue “Foundation Models for Medical Image Analysis” of the Medical Image Analysis journal. </div><div id="awards" data-v-0730d3ca><h2 data-v-0730d3ca>Awards</h2><hr data-v-0730d3ca><div class="awards-contents" data-v-0730d3ca> The MedAGI 2023 best paper and the honorable mention awards will be given to the two high-quality papers chosen by the award committee. </div></div><div id="submission-site" data-v-0730d3ca><h2 data-v-0730d3ca>Submission Site</h2><hr data-v-0730d3ca><a href="https://cmt3.research.microsoft.com/MedAGI2023" target="_blank" data-v-0730d3ca>https://cmt3.research.microsoft.com/MedAGI2023 </a></div></div></div>',1);function v(a,e,t,c,v,u){return(0,d.wg)(),(0,d.iD)("div",i,[s,"2023"===u.currentYear?((0,d.wg)(),(0,d.iD)("div",n,r)):((0,d.wg)(),(0,d.iD)("div",o," TBD ")),l])}var u={computed:{currentYear(){return this.$route.params.year||(new Date).getFullYear().toString()}},methods:{fetchData(){this.currentYear}},created(){this.fetchData()}},p=t(89);const m=(0,p.Z)(u,[["render",v],["__scopeId","data-v-0730d3ca"]]);var h=m}}]);
//# sourceMappingURL=746.c5e2f4e7.js.map